"""
RNN Text Generation Project
I will demonstrate how to generate text using a character based RNN. I will work with a dataset of Shakespeares writing.
Given a sequence if characters from this data (Shakespear), train a model to predict the next character in the sequence
(e). Longer sequences of text generated by calling the model repeatedly.

Setup
Import tensorflow and other libraries
"""

import tensorflow as tf
import keras.utils
import keras.layers
import numpy as np
import time
import os
import warnings
warnings.filterwarnings("ignore")


"""
Download Dataset
"""
path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')

# read in, then decode for py2 compat
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
# length of text is the number of characters in it
print("---------------------------------------------------------")
print(f'length in text: {len(text)} characters')

# print out first 250 characters in text
print("---------------------------------------------------------")
print(text[:250])

# get the unique characters in the file
vocab = sorted(set(text))
print("---------------------------------------------------------")
print(f'{len(vocab)} unique characters')

"""
Process The Text
Vectorize the text
Before training, I need to convert the strings to a numerical representation

The tf.keras.layers.StringLookup layer can convert each character into a numeric ID. Just to split up the text into 
tokens first.
"""

example_texts = ['abcdef', 'xyz']
chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')
print("---------------------------------------------------------")
print(chars)

"""
Now create the tk.keras.layers.StringLookup layer:
"""

ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)

# it converts from tokens to character IDs
ids = ids_from_chars(chars)
print("---------------------------------------------------------")
print(ids)

"""
Since the goal of this project is to generate text, it will also be important to invert this representation and recover 
human-readable strings from it. For this you can use tf.keras.layers.StringLookup(...invert=True)
"""

chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)

"""
This layer recovers the characters from the vectors of IDs, and returns them as a tf.RaggedTensor of characters
"""

chars = chars_from_ids(ids)
print("---------------------------------------------------------")
print(chars)

"""
You can tf.strings.reduce_join to join the characters back to strings
"""

# tf.strings.reduce_join(chars, axis=1).numpy()


def text_from_ids(ids):
    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)


"""
The Prediction Task 
Given a character, or a sequence of characters, what is the most probable next character? This is the task you're 
training the model to perform. The input to the model will be a sequence of characters, and you train the model to 
predict the output - the following character at each time step

Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed 
until this moment, what is the next character?

Create training examples and targets 
Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text

For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the
right.

So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is "Hello". The input 
sequence would be "Hell", and the target sequence "ello".

To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of 
character indices
"""

all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))
print("---------------------------------------------------------")
print(all_ids)

ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)

print("---------------------------------------------------------")
for ids in ids_dataset.take(10):
    print(chars_from_ids(ids).numpy().decode('utf-8'))

seq_length = 100
example_per_epoch = len(text)

sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)
print("---------------------------------------------------------")
for seq in sequences.take(1):
    print(chars_from_ids(seq))

"""
Its easier to see what this is doing if you join the tokens back into strings
"""

print("---------------------------------------------------------")
for seq in sequences.take(5):
    print(text_from_ids(seq).numpy())

"""
For training you'll need a dataset of (input, label) pairs. Where input and label are sequences. At each time step input
is the current character and the label is the next character.

Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each 
timestamp
"""


def split_input_target(sequence):
    input_text = sequence[:-1]
    target_text = sequence[1:]
    return input_text, target_text


print("---------------------------------------------------------")
print(split_input_target(list("tensorflow")))

dataset = sequences.map(split_input_target)

print("---------------------------------------------------------")
for input_example, target_example in dataset.take(1):
    print("input: ", text_from_ids(input_example).numpy())
    print("output: ", text_from_ids(target_example).numpy())

"""
Creating Training Batches

You used tf.data to split the text into manageable sequences. But before feeding this data into the model, you need to 
shuffle the data and pack it into batches
"""

# batch size
BATCH_SIZE = 64

# Buffer size to shuffle the dataset
# (TF data is designed to work with possibly infinite sequences so it doesn't attempt to shuffle the entire sequence in
# memory. Instead it maintains a buffer in which it shuffles elements)
BUFFER_SIZE = 10000

dataset = (dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))
print("---------------------------------------------------------")
print(dataset)

"""
Build The Model
This section defines the model as a keras.model subclass.

This model has three layers:

1. tf.keras.layers.Embedding: The input layer, a trainable lookup table that will map each character ID to a vector with
embedding_dim dimensions

2. tf.keras.layers.GRU: A type of RNN with size units=rnn_units

3. tf.keras.layers.Dense: The output layer, with vocab_size outputs one logit for each character in the vocabulary. 
These are the log-likelihood of each character according to the model
"""

# length of the vocabulary in chars
vocab_size = len(vocab)

# the embedding dimensions
embedding_dim = 256

# number of RNN units
rnn_units = 1024


class MyModel(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, rnn_units):
        super().__init__(self)
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)
        self.dense = tf.keras.layers.Dense(vocab_size)

    def call(self, inputs, states=None, return_state=False, training=False):
        x = inputs
        x = self.embedding(x, training=training)
        if states is None:
            states = self.gru.get_initial_state(x)
        x, states = self.gru(x, initial_state=states, training=training)
        x = self.dense(x, training=training)

        if return_state:
            return x, states
        else:
            return x


# be sure the vocabulary size matches the StringLookup layers.
model = MyModel(vocab_size=len(ids_from_chars.get_vocabulary()), embedding_dim=embedding_dim, rnn_units=rnn_units)

"""
For each character the model looks up the embedding, runs the GRU one time step with the embedding as input, and applies
the dense layer to generate logits predicting the log likelihood of the next character

Try The Model
Now run the model to see that it behaves as expected

First check the shape of the output
"""

print("---------------------------------------------------------")
for input_example_batch, target_example_batch in dataset.take(1):
    example_batch_predictions = model(input_example_batch)
    print(example_batch_predictions.shape, "# (batch_size, sequence_length, vocab_size)")

"""
In the above output the sequence length of the input is 100 but the model can be ran on inputs of any length
"""

print("---------------------------------------------------------")
# print(model.summary())

"""
To get actual predictions from the model you need to sample from the output distribution, to get actual character 
indices. This distribution is defined by the logits over the character vocabulary

The first example in the batch
"""

sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
sampled_indices = tf.squeeze(sampled_indices, axis=1).numpy()
print("---------------------------------------------------------")
print(sampled_indices)

"""
Decode these to see the text predicted by this untrained model
"""

print("---------------------------------------------------------")
print("input: \n", text_from_ids(input_example_batch[0]).numpy())
print()
print("next char prediction :\n", text_from_ids(sampled_indices).numpy())

"""
Train The Model
At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the 
input this time step, predict the class of the next character

Attach an optimizer, and a loss function
The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied 
across the last dimension of the prediction

Because your model returns logits, you need to set the from_logits flag
"""

loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)
example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)
print("---------------------------------------------------------")
print("prediction shape: ", example_batch_predictions.shape, "# (batch_size, sequence_length, vocab_size)")
print("mean loss: ", example_batch_mean_loss)

"""
A newly initialized model isn't too sure of itself, the output logits should all have similar magnitudes. To confirm
this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher 
loss means the model is sure of its wrong answers, and is badly initialized
"""

print("---------------------------------------------------------")
print(tf.exp(example_batch_mean_loss).numpy())

"""
Configure the training procedure using the tf.keras.Model.compile method. Use tf.keras.optimizers. Adam with default 
arguments and the loss function
"""

model.compile(optimizer='adam', loss=loss)

"""
Configure Checkpoints
Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training
"""

# the directory where the checkpoints will be saved
checkpoint_dir = './training_checkpoints'
# name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)

"""
Execute The Training 
To keep training time reasonable, use 10 epochs to train the model. I set the runtime to GPU for faster training
"""

EPOCHS = 20
#history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])









































